  * Last update: September 13 2019. 
# SparkChallenge01

### - Introduction: 
This is a challenge provided by Cloud Analytics 4 
Explained: 3 documentos con 3 columnas y tomando en consideración las columnas de cantidad y de precio se creará una nueva tabla (dataframe) con 2 columnas: 
Dataframe 1) qty y duplicados 
Dataframe 2) precio y duplicados

Metricas a evaluar:
performance con c/u de los núcleos 
tiempo de procesamiento con c/u
comportamiento 

Proceso:
Crear tests de c/u de los elementos que son evaluados, así como excepciones encontradas.
Evaluar el performance mediante la consola de Spark GUI localhost:4040

### Prerequisites
|             ----               |                         
| ------------------------------ |
| Github enterprise account      |
| VCS installed locally (Git)    |
| IntelliJ IDEA Community edition|
| IBM DB2 WoC account actived    | 
___
### - Tecnical Requirements:

* Apache Spark 2.3.0 version.
* Scala 2.11.12 version.
* Use Version Control System.
* Tests using FunSuite
* Add .gitignore file in repo.
* Encode credentials.

* Spark cores:

    | <10,000 = 1 Core
    
    | >= 10,001 && <= 30,000 = 2 Cores
    
    | > 30,000 = 30,000 = *
    
* Upload results to IBM Cloud.

1. Sold the same amount of products (+)
2. Same amount of sold dollars (*)

| ID | QTY | $$ |
| :-------- | :-------: | --------: |
| 1 | 1 (+) | $20 () |
| 2 | 3 | $70 |
| 3 | 2 | $20 () |
| 4 | 1 (+) | $100 |

___
### - Gherkin: 
- Scenario: 

  Given 

  When 
  
  Then 
  
  ___

- Scenario: 

  Given 
  
  When 
  
  Then 

___

- Scenario: 

  Given 
  
  When 
  
  Then 

___
### - Development:

___
### - Summary:
1.- Set up required development environments locally.

2.- Configure Github account with VCS.

3.- Request IBM Cloud access.

4.- Download project files from Box.

5.- Develop baseline project.

6.- Coding.

7.- 
___
### - Findings:

___
### - Conclusions:

